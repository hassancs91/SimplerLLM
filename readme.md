# âšª SimplerLLM (Beta)

âš¡ Your Easy Pass to Advanced AI âš¡


[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Join the Discord chat!](https://img.shields.io/badge/Join-Discord-7289DA.svg)](https://discord.gg/HUrtZXyp3j)



## ðŸ¤” What is SimplerLLM?

SimplerLLM is an open-source Python library designed to simplify interactions with Large Language Models (LLMs) for researchers and beginners. It offers a unified interface for different LLM providers and a suite of tools to enhance language model capabilities and make it Super easy for anyone to develop AI-powered tools and apps.

Below is a simple documentation, if you're looking for the whole detailed documentation [check the official website](https://docs.simplerllm.com/)

## Easy Installation

With pip:

```bash
pip install simplerllm
```

### Features

- **Unified LLM Interface**: Define an LLM instance in one line for providers like OpenAI, Google Gemini, Anthropic, and even Ollama. 
- **Generic Text Loader**: Load text from various sources like DOCX, PDF, TXT files, or blog posts.
- **RapidAPI Connector**: Connect with AI services on RapidAPI.
- **SERP Integration**: Perform searches easily using Serper and Value Serp APIs.
- **Prompt Template Builder**: Easily create and manage prompt templates.
  And Much More Coming Soon!

### Setting Up Environment Variables

To use this library, you need to set several API keys in your environment. Start by creating a .env file in the root directory of your project and adding your API keys there.

ðŸ”´ This file should be kept private and not committed to version control to protect your keys.

Here is an example of what your .env file should look like:

```
OPENAI_API_KEY="your_openai_api_key_here" # For accessing OpenAI's API
GEMINI_API_KEY="your_gemeni_api_key_here" # For accessing Gemini's API
ANTHROPIC_API_KEY="your_claude_api_key_here" # For accessing Anthropic's API
RAPIDAPI_API_KEY="your_rapidapi_api_key_here" # For accessing APIs on RapidAPI
VALUE_SERP_API_KEY="your_value_serp_api_key_here" # For Google search
SERPER_API_KEY="your_serper_api_key_here" # For Google search
STABILITY_API_KEY="your_stability_api_key_here" # For image generation
```

## Creating an LLM Instance

```python
from SimplerLLM.language.llm import LLM, LLMProvider

# For OpenAI
llm_instance = LLM.create(provider=LLMProvider.OPENAI, model_name="gpt-3.5-turbo")

# For Google Gemini
#llm_instance = LLM.create(provider=LLMProvider.GEMINI, model_name="gemini-1.5-flash")

# For Anthropic Claude
#llm_instance = LLM.create(provider=LLMProvider.ANTHROPIC, model_name="claude-3-5-sonnet-20240620")

# For Ollama (Local Model)
#llm_instance = LLM.create(provider=LLMProvider.OLLAMA, model_name="phi")

# Generate a response
response = llm_instance.generate_response(prompt="generate a 5 words sentence")
print(response)
```

## Generating a JSON easily with any LLM instance

This function helps you always get a json structured response from LLMs. This will help you a lot if you're using the response for your software and you want a stable json output.

```python
from pydantic import BaseModel
from SimplerLLM.language.llm import LLM, LLMProvider
from SimplerLLM.language.llm_addons import generate_pydantic_json_model

class LLMResponse(BaseModel):
    response: str

llm_instance = LLM.create(provider=LLMProvider.OPENAI, model_name="gpt-4o")
prompt = "generate a sentence about the importance of AI"

output = generate_pydantic_json_model(llm_instance=llm_instance,prompt=prompt,model_class=LLMResponse)
json_output = output.model_dump()
```

The `output` generated by the LLM in this case will be an object of type LLMResponse, and to parse it easily into a json response we use the `model_dump()` function.

## Using Tools

### SERP

```python
from SimplerLLM.tools.serp import search_with_serper_api

search_results = search_with_serper_api("your search query", num_results=3)

# use the search results the way you want!

```

### Generic Text Loader

```python
from SimplerLLM.tools.generic_loader import load_content

text_file = load_content("file.txt")

print(text_file.content)

```

### Calling any RapidAPI API

```python
from  SimplerLLM.tools.rapid_api import RapidAPIClient

api_url = "https://domain-authority1.p.rapidapi.com/seo/get-domain-info"
api_params = {
    'domain': 'learnwithhasan.com',
}

api_client = RapidAPIClient()  # API key read from environment variable
response = api_client.call_api(api_url, method='GET', params=api_params)
```

### Prompt Template Builder

```python
from SimplerLLM.prompts.prompt_builder import create_multi_value_prompts,create_prompt_template

basic_prompt = "Generate 5 titles for a blog about {topic} and {style}"

prompt_template = pr.create_prompt_template(basic_prompt)

prompt_template.assign_parms(topic = "marketing",style = "catchy")

print(prompt_template.content)


## working with multiple value prompts
multi_value_prompt_template = """Hello {name}, your next meeting is on {date}.
 and bring a {object} wit you"""

params_list = [
     {"name": "Alice", "date": "January 10th", "object" : "dog"},
     {"name": "Bob", "date": "January 12th", "object" : "bag"},
     {"name": "Charlie", "date": "January 15th", "object" : "pen"}
]


multi_value_prompt = create_multi_value_prompts(multi_value_prompt_template)
generated_prompts = multi_value_prompt.generate_prompts(params_list)

print(generated_prompts[0])

```

## Chunking Functions

We have introduced new functions to help you split texts into manageable chunks based on different criteria. These functions are part of the chunker tool.

### chunk_by_max_chunk_size

This function splits text into chunks with a maximum size, optionally preserving sentence structure.

### chunk_by_sentences

This function splits the text into chunks based on sentences.

### chunk_by_paragraphs

This function splits text into chunks based on paragraphs.

### chunk_by_semantics

This functions splits text into chunks based on semantics.

Example

```python
from SimplerLLM.tools.text_chunker import chunk_by_semantics
from SimplerLLM.language.embeddings import EmbeddingsLLM, EmbeddingsProvider

blog_url = "https://www.semrush.com/blog/digital-marketing/"
blog_post = loader.load_content(blog_url)
text = blog_post.content

embeddings_model = EmbeddingsLLM.create(provider=EmbeddingsProvider.OPENAI,
                                        model_name="text-embedding-3-small")
semantic_chunks = chunk_by_semantics(text, embeddings_model, threshold_percentage=80)

print(semantic_chunks)
```

## Next Updates

- Adding More Tools
- Prompt Optimization
- Response Evaluation
- GPT Trainer
- Advanced Document Loader
- Integration With More Providers
- Simple RAG With SimplerVectors
- Integration with Vector Databases
- Agent Builder
- LLM Server 
